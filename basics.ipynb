{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install pickle\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Laden Sie die Umgebungsvariablen aus der .env-Datei\n",
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders  \n",
    "Um Daten mit einem LLM zu verwenden, müssen Dokumente zunächst in eine Vectordatenbank. \n",
    "Der erste Schritt ist diese über einen Loader in memory zu laden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader('./FAQ', glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitter\n",
    "Texte werden nicht 1:1 in die Datenbank geladen, sondern in Stücken, sog. \"Chunks\". Man kann die Chunk Größe und den Overlap zwischen den Chunks definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Texte werden nicht als Text in der Datenbank gespeichert, sondern als Vectorrepräenstation.\n",
    "Embeddings sind eine Art von Wortdarstellung, die die semantische Bedeutung von Wörtern in einem Vektorraum darstellt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Vectoren in VectorDB (FAISS)\n",
    "Wie von OpenAIEmbeddings erstellen Vectoren können nun in der Datenbank gespeichert. Die DB kann als .pkl file abgelegt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.faiss import FAISS\n",
    "import pickle\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "with open(\"vectorstore.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden der Datenbank\n",
    "Vor der Verwendung der Datenbank muss diese natürlich wieder geladen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectorstore.pkl\", \"rb\") as f:\n",
    "    vectorstore = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "Bei einem LLM hat man die Möglichkeit, diesem vor einer Konversersation eine Identität zu verpassen oder zu definieren wie Frage und Antwort aussehen sollen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Du bist ein Tierarzt, Usern beim Umgang mit ihrem Tier .\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Antwort hier:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains\n",
    "Mit Chain Klassen kann man das Verhalten des LLMs leicht beeinflussen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "llm = OpenAI(openai_api_key=API_KEY)\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "query = \"Wie alt wird ein Huninhen?\"\n",
    "qa.run(query)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "In dem eben gezeigten Beispiel steht jede Anfrage für sich. Eine große Stärke eines LLM ist allerdings, dass diese bei einer Antwort den kompletten Chatverlauf berücksichtigen kann. Dafür muss allerdings aus den unterschiedlichen Fragen und Antworten eine Chathistorie aufgebaut werden. Mit unterschiedlichen Memory Klassen ist dies in Langchain sehr einfach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory in Chains verwenden\n",
    "Die Memory Klasse kann nun einfach in einer Chain verwendet werden. Erkennbar ist dies z.B. daran, dass wenn man von \"es\" spricht, der Bot das Huninchen in diesem Kontext versteht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=OpenAI(model_name=\"text-davinci-003\", temperature=0.7, openai_api_key=API_KEY),\n",
    "    memory=memory,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_docs_chain_kwargs={'prompt': PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "query = \"Wie alt wird ein Huninhen?\"\n",
    "qa({\"question\": query})\n",
    "qa({\"question\": \"Und wie viel frisst es?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
